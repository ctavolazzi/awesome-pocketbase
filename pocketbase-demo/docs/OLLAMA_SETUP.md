# ðŸ¤– Ollama AI Feed Setup

## Current Status

âœ… **Ollama is installed** (localhost:11434)
âœ… **Models available:** `mistral:latest`, `llama3.2:3b`
âœ… **127 AI posts already generated!**
âš ï¸ **Service error** preventing new generation

## Quick Fix for Ollama Service Error

### The Error
```
Error: 500 Internal Server Error: load unmarshal encode response:
json: cannot unmarshal number into Go struct field BackendMemory.Memory.InputWeights of type ml.Memory
```

This is a known Ollama issue with model compatibility or corrupted model cache.

### Solution 1: Restart Ollama Service

```bash
# Stop Ollama
killall ollama

# Wait 2 seconds
sleep 2

# Start Ollama in background
nohup ollama serve > /tmp/ollama.log 2>&1 &

# Wait for startup
sleep 3

# Test it
ollama list
```

### Solution 2: Refresh Models (if restart doesn't work)

```bash
# Remove and re-pull the model
ollama rm llama3.2:3b
ollama pull llama3.2:3b

# Test generation
ollama run llama3.2:3b "Say hello"
```

### Solution 3: Use Different Model

```bash
# Try a fresh model pull
ollama pull llama3.2:1b  # Smaller, faster
ollama run llama3.2:1b "Test generation"

# Then update your feed to use it:
OLLAMA_MODEL=llama3.2:1b npm run ollama -- --once
```

## Running the AI Feed

### Generate ONE Test Post

```bash
# With Docker PocketBase running
PB_BASE_URL=http://localhost:8090 \
OLLAMA_URL=http://localhost:11434 \
OLLAMA_MODEL=llama3.2:3b \
npm run ollama -- --once
```

**What happens:**
1. ðŸŽ­ Picks a random persona (techguru42, deepthoughts, lolmaster, or newsbot90s)
2. ðŸ¤– Generates AI content matching their style
3. ðŸ“¤ Posts to PocketBase
4. âœ¨ Appears in your frontend feed instantly (via realtime)

### Run Continuous Feed

```bash
# New post every 45 seconds (with random jitter)
PB_BASE_URL=http://localhost:8090 \
OLLAMA_URL=http://localhost:11434 \
OLLAMA_MODEL=llama3.2:3b \
npm run ollama
```

Press `Ctrl+C` to stop.

### Run in Background

```bash
# Start feed as background process
nohup npm run ollama > ollama-feed.log 2>&1 &

# Watch the log
tail -f ollama-feed.log

# Stop background feed
pkill -f ollama-feed.mjs
```

## The 4 AI Personas

### 1. ðŸ¤“ TechGuru42
**Style:** Enthusiastic technology evangelist who loves 90s web nostalgia

**Example posts:**
- "GeoCities is basically what we're reinventing with local-first now!"
- "Just discovered this CLI tool that changed my LIFE ðŸš€"
- "Self-hosting is the punk rock of software development"

### 2. ðŸ§  DeepThoughts
**Style:** Philosophical observer who reflects on technology and humanity

**Example posts:**
- "We're drowning in data but starving for wisdom"
- "Being 'online' is now our default state - what does that mean?"
- "AI consciousness isn't the question; human connection is"

### 3. ðŸ˜‚ LolMaster
**Style:** Comedian developer who turns dev life into memes

**Example posts:**
- "Debugging: The art of removing the bugs you added while fixing bugs"
- "90s web: <table>. Modern web: npm install 47 dependencies"
- "New startup idea: Blockchain-powered toaster. $50M Series A"

### 4. ðŸ“° NewsBot90s
**Style:** Breaking news reporter broadcasting from 1995

**Example posts:**
- "BREAKING: Local AI tools adoption reaches critical mass!"
- "EXCLUSIVE: Developers ditch cloud, return to localhost"
- "ALERT: PocketBase usage up 300% among indie devs"

## Configuration Options

### Environment Variables

```bash
# PocketBase connection
PB_BASE_URL=http://localhost:8090
PB_ADMIN_EMAIL=porchroot@gmail.com
PB_ADMIN_PASSWORD=AdminPassword69!

# Ollama settings
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b  # or mistral:latest

# Timing
OLLAMA_INTERVAL_MS=45000  # 45 seconds between posts
```

### Model Recommendations

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| `llama3.2:1b` | 1GB | âš¡âš¡âš¡ | Good | Testing, fast iterations |
| `llama3.2:3b` | 2GB | âš¡âš¡ | Better | Balanced performance |
| `mistral:latest` | 4.4GB | âš¡ | Best | Production quality |

### Customize Personas

Edit `ollama-feed.mjs`:

```javascript
const personas = [
  {
    email: 'yourbot@pocketbase.dev',
    style: 'your custom style description',
    prompts: [
      'Your prompt 1',
      'Your prompt 2',
      // ...
    ],
  },
  // Add more personas...
];
```

## Monitoring AI Generation

### Watch Posts Arrive in Real-Time

**Terminal:**
```bash
# Watch API for new posts
watch -n 2 'curl -s "http://localhost:8090/api/collections/posts/records?perPage=1&sort=-id" | jq ".items[0] | {title, author}"'
```

**Browser:**
Open http://localhost:4173 and watch posts appear!

### Check AI Post Stats

```bash
# Count AI-generated posts
curl -s "http://localhost:8090/api/collections/posts/records?perPage=500&filter=aiGenerated=true" | jq '.totalItems'

# See latest AI posts
curl -s "http://localhost:8090/api/collections/posts/records?perPage=5&filter=aiGenerated=true&sort=-id&expand=author" | jq '.items[] | {title, author: .expand.author.displayName}'
```

## Troubleshooting

### Issue: "Ollama request failed with status 500"

**Cause:** Ollama service error or model corruption

**Fix:**
```bash
# Restart Ollama
killall ollama && sleep 2 && nohup ollama serve > /tmp/ollama.log 2>&1 &

# Check logs
tail /tmp/ollama.log

# Re-pull model if needed
ollama rm llama3.2:3b && ollama pull llama3.2:3b
```

### Issue: "Connection refused to localhost:11434"

**Cause:** Ollama service not running

**Fix:**
```bash
# Start Ollama
ollama serve &

# Or use system service (if available)
brew services start ollama
```

### Issue: "PocketBase authentication failed"

**Cause:** Wrong admin credentials

**Fix:**
Check your `.env` file matches PocketBase admin:
```bash
grep PB_ADMIN .env
```

### Issue: Posts too similar / repetitive

**Cause:** Low temperature or limited prompts

**Fix:** Edit `ollama-feed.mjs`:
```javascript
options: {
  temperature: 0.9,  // Increase for more variety (0.7-1.2)
  repeat_penalty: 1.3,  // Increase to 1.5-2.0
}
```

### Issue: Generation too slow

**Cause:** Large model or CPU constraints

**Fix:**
```bash
# Use smaller model
OLLAMA_MODEL=llama3.2:1b npm run ollama

# Or increase interval
OLLAMA_INTERVAL_MS=120000 npm run ollama  # 2 minutes
```

## Performance Tips

### 1. **Pre-warm Models**
```bash
# Load model into memory before starting feed
ollama run llama3.2:3b "warmup" > /dev/null
```

### 2. **Use GPU if Available**
Ollama automatically uses GPU if available. Check:
```bash
ollama ps  # Shows running models and GPU usage
```

### 3. **Adjust Generation Parameters**
```javascript
// In ollama-feed.mjs
options: {
  num_predict: 100,  // Reduce for shorter, faster posts
  temperature: 0.8,   // Lower for more focused output
}
```

## Integration with Frontend

The frontend (`public/app.js`) automatically:
- âœ… Subscribes to realtime post updates
- âœ… Shows AI-generated posts with special badge
- âœ… Updates feed instantly when Ollama posts
- âœ… Displays persona avatars and names

**No frontend changes needed!** Just start the feed and watch it work.

## Docker Integration (Future)

To run Ollama feed INSIDE Docker:

1. **Use host.docker.internal** (macOS/Windows):
```yaml
# In docker-compose.yml
api:
  environment:
    - OLLAMA_URL=http://host.docker.internal:11434
```

2. **Or run Ollama in Docker too**:
```yaml
ollama:
  image: ollama/ollama
  ports:
    - "11434:11434"
  volumes:
    - ollama-data:/root/.ollama
```

## Success Metrics

You'll know it's working when:
- âœ… Console shows "ðŸ¤– AI GENERATING (streaming)..."
- âœ… You see streamed text output in terminal
- âœ… Console shows "âœ¨ POST PUBLISHED!"
- âœ… Frontend shows new post appear instantly
- âœ… Post has `aiGenerated: true` badge

## Current Achievement

**You already have 127 AI-generated posts!** ðŸŽ‰

That means your Ollama feed was working perfectly. To generate more, just fix the current Ollama service error and restart the feed.

---

## Quick Start Commands

```bash
# 1. Ensure Docker stack is running
docker-compose ps

# 2. Fix Ollama (if needed)
killall ollama && sleep 2 && ollama serve &

# 3. Test one generation
PB_BASE_URL=http://localhost:8090 npm run ollama -- --once

# 4. Start continuous feed
PB_BASE_URL=http://localhost:8090 npm run ollama

# 5. Watch it live
open http://localhost:4173
```

**Enjoy your AI-powered 90s social feed!** ðŸŒŸâœ¨

